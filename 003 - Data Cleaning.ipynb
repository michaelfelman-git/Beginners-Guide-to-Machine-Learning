{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe1ee60",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">DATA CLEANING</h1>\n",
    "<h2 align=\"left\"><ins>Contents</ins></h2>\n",
    "\n",
    "- [**DATA CLEANING**](#intro)\n",
    "    - [**Common Data Wrangling Methods**](#wrangle)\n",
    "        - [**Descriptive Statistics**](#desc)\n",
    "        - [**Set Index**](#index)\n",
    "        - [**Replace Values**](#replace)\n",
    "        - [**Rename Columns**](#rename)\n",
    "        - [**Unique**](#unique)\n",
    "        - [**Handling Missing Values**](#nulls)\n",
    "        - [**Drop rows or columns**](#drop)\n",
    "    - [**Identify & Remove Columns That Contain a Single Value**](#single)\n",
    "    - [**Consider Columns That Have Very Few Unique Values**](#few)\n",
    "    - [**Remove Columns That Have A Low Variance**](#variance)\n",
    "    - [**Identify & Remove Rows That Contain Duplicate Data**](#duplicate)\n",
    "- [**OUTLIER IDENTIFICATION AND REMOVAL**](#outlier)\n",
    "    - [**What Are Outliers?**](#what)\n",
    "    - [**How To Handle Outliers**](#handle)\n",
    "    - [**Detecting Outliers**](#detect)\n",
    "        - [**Elliptic Envelope Method**](#detect)\n",
    "        - [**Interquartile Range Method**](#iqr)\n",
    "        - [**Standard Deviation Method**](#std)\n",
    "        - [**Automatic Outlier Detection**](#auto)\n",
    "- [**HOW TO HANDLE MISSING DATA**](#missing)\n",
    "    - [**Identify & Mark Missing Values**](#identify)\n",
    "    - [**Remove Rows With Missing Values**](#remove)\n",
    "    - [**How to Use Statistical Imputation**](#impute)\n",
    "        - [**Using Pandas to Fill Missing Values**](#fill)\n",
    "        - [**SimpleImputer Data Transform**](#transform)\n",
    "        - [**SimpleImputer and Model Evaluation**](#eval)\n",
    "        - [**Comparing Different Imputed Statistics**](#compare)\n",
    "        - [**SimpleImputer Transform When Making a Prediction**](#predict)\n",
    "    - [**How to Use KNN Imputation**](#knn)\n",
    "        - [**KNNImputer Data Transform**](#data)\n",
    "        - [**KNNImputer and Model Evaluation**](#model)\n",
    "        - [**KNNImputer and Different Number of Neighbors**](#range)\n",
    "        - [**KNNImputer Transform When Making a Prediction**](#make)\n",
    "    - [**How to Use Iterative Imputation**](#iter)\n",
    "        - [**IterativeImputer Data Transform**](#iidt)\n",
    "        - [**IterativeImputer and Model Evaluation**](#ii_model)\n",
    "        - [**IterativeImputer and Different Imputation Order**](#order)\n",
    "        - [**IterativeImputer and Different Number of Iterations**](#number)\n",
    "        - [**IterativeImputer Transform When Making a Prediction**](#ii_pred)\n",
    "- [**REFERENCES**](#ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338645b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a17f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, KFold, \n",
    "                                     cross_val_score, RepeatedStratifiedKFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1a680",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "<h1 align=\"center\">DATA CLEANING</h1>\n",
    "\n",
    "**Data cleaning** refers to identifying and correcting errors in the dataset that may negatively impact a predictive model. Data cleaning is a critically important step in any machine learning project. \n",
    "\n",
    "In tabular data, there are many different statistical analysis and data visualization techniques used to explore the data in order to identify any data cleaning operations that may be required. There are many types of errors that exist in a dataset, although some of the simplest errors include columns that don’t contain much information and duplicated rows. Although critically important, it is neither exciting, nor does it involve fancy techniques. Just a good knowledge of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d26bd0",
   "metadata": {},
   "source": [
    "<a id=\"wrangle\"></a>\n",
    "<h3 style=\"text-decoration:underline\">Common Data Wrangling Methods</h3>\n",
    "\n",
    "**Data wrangling** is a broad term often used (informally) to describe the process of transforming raw data to a clean and organized format ready for use. Data wrangling is only one step in preprocessing data, but it is an important step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678a2893",
   "metadata": {},
   "source": [
    "<a id=\"desc\"></a>\n",
    "After loading the data, it is a good idea to understand how it is structured and what kind of information it contains.\n",
    "\n",
    "```python\n",
    "# Show dimensions (rows, columns)\n",
    "df.shape\n",
    "\n",
    "# Show basic descriptive statistics\n",
    "df.describe(percentiles=None, include=None, exclude=None, datetime_is_numeric=False)\n",
    "\n",
    "# show data types\n",
    "df.info()\n",
    "```\n",
    "[Pandas Descriptive Statistics Docs](https://pandas.pydata.org/docs/user_guide/basics.html#descriptive-statistics)\n",
    ">It is worth noting that summary statistics do not always tell the full story. For example, encoded categorical variables will appear as numerical columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b5e92",
   "metadata": {},
   "source": [
    "<a id=\"index\"></a>\n",
    "All rows in a pandas DataFrame have a unique index value. By default, this index is an integer indicating the row position in the DataFrame. However, dataFrames do not need to be numerically indexed. We can **set the index** of a DataFrame to any value where the value is unique to each row - either a string or number.\n",
    "\n",
    "```python\n",
    "df.set_index(keys, drop=True, append=False, \n",
    "             inplace=False, verify_integrity=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7c2be",
   "metadata": {},
   "source": [
    "<a id=\"replace\"></a>\n",
    "Pandas `replace` is an easy way to find and replace values.\n",
    "\n",
    "```python\n",
    "# Replace value in a single column\n",
    "df['column_name'].replace(\"val_to_find\", \"to_replace\")\n",
    "\n",
    "# replace multiple values at the same time in a single column\n",
    "df['column_name'].replace([\"val_to_find_1\", \"val_to_find_2\"],\n",
    "                          [\"to_replace_1\", \"to_replace_2\"])\n",
    "\n",
    "# replace multiple values at the same time to entire dataframe\n",
    "df.replace({\"val_to_find_1\": \"to_replace_1\",\n",
    "            \"val_to_find_2\": \"to_replace_2\"})\n",
    "\n",
    "# search within multiple columns\n",
    "df.replace({'col_name_1': \"val_to_find_1\", 'col_name_2': \"val_to_find_2\"}, \"to_replace\")\n",
    "\n",
    "# replace multiple values within a column\n",
    "df.replace({'col_name_1': {\"val_to_find_1\": \"to_replace_1\", \"val_to_find_2\": \"to_replace_2\"}})\n",
    "\n",
    "# using regular expression\n",
    "df.replace(to_replace=r'reg_ex', value='to_replace', regex=True)\n",
    "\n",
    "df.replace({'col_name': r'reg_ex'}, {'col_name': 'to_replace'}, regex=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950395cc",
   "metadata": {},
   "source": [
    "<a id=\"rename\"></a>\n",
    "**Rename** columns: \n",
    "\n",
    "```python \n",
    "df.columns = [f'Col_{i}' for i in range(1, df.shape[1])]\n",
    "df.rename(columns={'old_name':'new_name'}, inplace=True)\n",
    "          columns=lambda x: f\"Col_{int(x)+1}\"\n",
    "df.insert(loc, 'column_name', values)\n",
    "```\n",
    "\n",
    "To create a dictionary with the old column names as keys and empty strings as values:\n",
    "<code>col_dict = {i:'' for i in df.columns}</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b9066",
   "metadata": {},
   "source": [
    "<a id=\"unique\"></a>\n",
    "Both **unique** and **value_counts** are useful for manipulating and exploring categorical columns. Very often in categorical columns there will be classes that need to be handled in the data wrangling phase.\n",
    "\n",
    "```python\n",
    "# Count distinct observations\n",
    "df[\"col_name\"].nunique(axis=0, dropna=True)\n",
    "\n",
    "# return distinct values in a column\n",
    "df[\"col_name\"].unique()\n",
    "\n",
    "df[\"col_name\"].value_counts()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6a636b",
   "metadata": {},
   "source": [
    "<a id=\"nulls\"></a>\n",
    "**Missing values** are a ubiquitous problem in data wrangling, yet many\n",
    "underestimate the difficulty of working with missing data.\n",
    "\n",
    "Besides using `df.info()`, we can also do:\n",
    "```python\n",
    "# Return boolean values for each column.\n",
    "df.isnull().any() \n",
    "\n",
    "# Show the number of missing values for each column.\n",
    "df.isnull().sum()\n",
    "\n",
    "# Show the total number of missing values in the dataframe.\n",
    "df.isnull().sum().sum()\n",
    "\n",
    "# A nice visual representation of the missing values.\n",
    "sns.heatmap(df.isnull(),cbar=False) \n",
    "```\n",
    ">**Though not always recommended, it is okay to drop missing rows if less than 5%.**\n",
    "\n",
    "There is also the `missingo` python library which has some nice visualizations.\n",
    "```python\n",
    "import missingno as msno\n",
    "\n",
    "msno.bar(df)\n",
    "msno.matrix(df)\n",
    "msno.heatmap(df)\n",
    "msno.dendrogram(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e4c5e4",
   "metadata": {},
   "source": [
    "<a id=\"drop\"></a>\n",
    "**Drop a row (axis=0) or column (axis=1)**:\n",
    "```python\n",
    "df.drop(labels=index/row_label, inplace=True)\n",
    "df.drop(columns=['column_names'], inplace=True)\n",
    "df.drop([row_labels or 'columns'], axis=0 or 1, inplace=True)\n",
    "del df['column_name']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb069c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd29be5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/titanic.csv'\n",
    "# Load data\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d1eb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>PClass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>SexCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allen, Miss Elisabeth Walton</td>\n",
       "      <td>1st</td>\n",
       "      <td>29.00</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allison, Miss Helen Loraine</td>\n",
       "      <td>1st</td>\n",
       "      <td>2.00</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allison, Mr Hudson Joshua Creighton</td>\n",
       "      <td>1st</td>\n",
       "      <td>30.00</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Allison, Mrs Hudson JC (Bessie Waldo Daniels)</td>\n",
       "      <td>1st</td>\n",
       "      <td>25.00</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allison, Master Hudson Trevor</td>\n",
       "      <td>1st</td>\n",
       "      <td>0.92</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Name PClass    Age     Sex  \\\n",
       "0                   Allen, Miss Elisabeth Walton    1st  29.00  female   \n",
       "1                    Allison, Miss Helen Loraine    1st   2.00  female   \n",
       "2            Allison, Mr Hudson Joshua Creighton    1st  30.00    male   \n",
       "3  Allison, Mrs Hudson JC (Bessie Waldo Daniels)    1st  25.00  female   \n",
       "4                  Allison, Master Hudson Trevor    1st   0.92    male   \n",
       "\n",
       "   Survived  SexCode  \n",
       "0         1        1  \n",
       "1         0        1  \n",
       "2         0        0  \n",
       "3         0        1  \n",
       "4         1        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88809d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b497782a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d4efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb69e2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fbb42e0",
   "metadata": {},
   "source": [
    "<a id=\"single\"></a>\n",
    "<h3 style=\"text-decoration:underline\">Identify & Remove Columns That Contain a Single Value</h3>\n",
    "\n",
    "Columns that have a single observation or value are probably useless for modeling. These columns or predictors are referred to as *zero-variance predictors* - if we measured the variance (average value from the mean), it would be zero. \n",
    ">**Columns that have a single value for all rows do not contain any information for modeling. Depending on the choice of data preparation and modeling algorithms, variables with a single value can also cause errors or unexpected results.**\n",
    ">>**Variables or columns that have a single value should probably be removed from your dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06facfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using numpy \n",
    "# data = np.loadtxt('./datasets/Data Cleaning/oil_spill.csv', delimiter=',')\n",
    "\n",
    "# using pandas\n",
    "df = pd.read_csv('./datasets/Data Cleaning/oil_spill.csv',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column-wise unique value count\n",
    "print(df.nunique().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc51e01",
   "metadata": {},
   "source": [
    "We can see that the column at index 22 only has a single value (all are zeros). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cef2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.nunique().loc[lambda x : x==1]\n",
    "# df.nunique()[lambda x: x==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "print(f'Original shape is: {df.shape}')\n",
    "\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
    "print(f'Columns to delete are: {to_del}')\n",
    "\n",
    "# drop useless columns\n",
    "df.drop(to_del, axis=1)\n",
    "# df.drop(columns=[22], inplace=True)\n",
    "\n",
    "print(f'New shape after removal is: {df.drop(to_del, axis=1).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25647f61",
   "metadata": {},
   "source": [
    "<a id=\"few\"></a>\n",
    "<h3 style=\"text-decoration:underline\">Consider Columns That Have Very Few Unique Values</h3>\n",
    "\n",
    "There are some columns in the dataset that have less than 10 unique values. This might make sense for ordinal or categorical variables. However, in this case, the dataset only contains numerical variables. As such, only having 2, 4, or 9 unique numerical values in a column might be surprising. We can refer to these columns or predictors as *near-zero variance predictors*, as their variance is not zero, but a very small number close to zero.\n",
    ">**These columns may or may not contribute to the performance of a model.** We can’t assume that they are useless to modeling. Depending on the choice of data preparation and modeling algorithms, variables with very few numerical values can also cause errors or unexpected results. \n",
    "\n",
    "To help highlight columns of this type, we can calculate the number of unique values for each variable as a percentage of the total number of rows in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14325f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].nunique() > 10:\n",
    "#         print(f'The column {col} has {df[col].nunique()} unique values and is good to go')\n",
    "        continue\n",
    "    else:\n",
    "        print(f'The column {col} has {df[col].nunique()} unique values which are:\\n{df[col].unique()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa0866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(data=df.nunique(),columns=['Unique Count'],\n",
    "                    index=df.columns)\n",
    "\n",
    "# calculate the percentage of unique values to the total number of rows\n",
    "test['% of total_rows'] = test['Unique Count'].apply(lambda x: (x/df.shape[0])*100)\n",
    "\n",
    "# only include columns that have less than 10 unique values\n",
    "test[test['Unique Count']<10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b1d3f",
   "metadata": {},
   "source": [
    "We can see that 11 of the 50 numerical variables have unique values that are less than 1 percent of the number of rows. This does not mean that these rows and columns should be deleted, but they require further attention. \n",
    "- Perhaps the unique values can be encoded as ordinal values.\n",
    "- Perhaps the unique values can be encoded as categorical values.\n",
    "- Perhaps compare model performance with each variable removed from the dataset.\n",
    "\n",
    "For example, if we wanted to delete all 10 columns with unique values less than 1 percent of rows:\n",
    "\n",
    "```python\n",
    "counts = df.nunique()\n",
    "to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0]*100) < 1]\n",
    "         [i for i,v in counts.items() if (v/df.shape[0]*100) < 1]\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.nunique().loc[lambda x : x==1]\n",
    "# data.df.nunique()[lambda x: x==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e8e178",
   "metadata": {},
   "source": [
    "<a id=\"variance\"></a>\n",
    "<h3 style=\"text-decoration:underline\">Remove Columns That Have A Low Variance</h3>\n",
    "\n",
    "Another approach to the problem of removing columns with few unique values is to consider the variance of the column. Recall that the variance is a statistic calculated as the average squared difference of values in the sample from its mean. \n",
    ">The variance can be used as a filter for identifying columns to be removed from the dataset. A column that has a single value has a variance of 0.0, and a column that has very few unique values may have a small variance.\n",
    "\n",
    "One approach is using numpy and pandas:\n",
    "\n",
    "```python\n",
    "variance = np.var(df)\n",
    "low_variance = [i for i,v in variance.items() if v < threshold]\n",
    "df.drop(low_variance, axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "Another approach is the `VarianceThreshold` class from the scikit-learn library. An instance of the class can be created and we can specify the threshold argument, which defaults to 0.0 to remove columns with a single value. It can then be fit and applied to a dataset by calling the `fit_transform()` function to create a transformed version of the dataset where the columns that have a variance lower than the threshold have been removed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf160e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# define the transform\n",
    "transform = VarianceThreshold()\n",
    "\n",
    "# transform the input data\n",
    "X_sel = transform.fit_transform(X)\n",
    "print(X_sel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473656eb",
   "metadata": {},
   "source": [
    "The transform (with a default threshold of 0.0) has removed all columns with a variance of 0.0 - i.e. the single column where all values are the same (column 22).\n",
    "\n",
    "We can also see what happens when we use different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f75d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define thresholds to check\n",
    "thresholds = np.arange(0.0, 0.55, 0.05)\n",
    "\n",
    "# apply transform with each threshold\n",
    "results = list()\n",
    "for t in thresholds:\n",
    "    \n",
    "    # define the transform\n",
    "    transform = VarianceThreshold(threshold=t)\n",
    "    \n",
    "    # transform the input data\n",
    "    X_sel = transform.fit_transform(X)\n",
    "    \n",
    "    # determine the number of input features\n",
    "    n_features = X_sel.shape[1]\n",
    "    \n",
    "    # store the result\n",
    "    results.append(n_features)\n",
    "    \n",
    "    print('>Threshold=%.2f, Features=%d, #_cols_removed=%d' % (t, n_features,X.shape[1]-n_features)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1e5af",
   "metadata": {},
   "source": [
    "We can see that the number of features in the dataset quickly drops from 48 in the unchanged data down to 35 with a threshold of 0.15. It later drops to 31 (18 columns deleted) with a threshold of 0.5. We can illustrate the relationship between the threshold and the number of features in the transformed dataset using a line plot. Even with a small threshold between 0.15 and 0.4, a large number of features are removed immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb98de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds, results)\n",
    "plt.xlabel('Threshold',fontsize=14)\n",
    "plt.ylabel('Number of Features',fontsize=14)\n",
    "plt.title('Threshold v Number of Features', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0df67",
   "metadata": {},
   "source": [
    "<a id=\"duplicate\"></a>\n",
    "<h3 style=\"text-decoration:underline\">Identify & Remove Rows That Contain Duplicate Data</h3>\n",
    "\n",
    "Rows that have identical data are useless to the modeling process, if not dangerously misleading during model evaluation. Here, a duplicate row is a row where each value in each column for that row appears in identically the same order (same column values) in another row. Machine learning algorithms will perform better by identifying and removing rows with duplicate data. From an algorithm evaluation perspective, duplicate rows will result in misleading performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = pd.read_csv('./datasets/Data Cleaning/iris_flowers.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273efbe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_iris.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fea38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris[df_iris.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b55180f",
   "metadata": {},
   "source": [
    "Rows of duplicate data should probably be deleted from your dataset prior to modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac1ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original Shape: {df_iris.shape}')\n",
    "\n",
    "# delete duplicate rows\n",
    "df_iris.drop_duplicates(inplace=True)\n",
    "print(f'After removing duplicates: {df_iris.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23a8fe",
   "metadata": {},
   "source": [
    "<a id=\"outlier\"></a>\n",
    "<h1 align=\"center\">OUTLIER IDENTIFICATION AND REMOVAL</h1>\n",
    "\n",
    "When modeling, it is important to clean the data to ensure the observations best represent the problem. Sometimes data can contain extreme values that are outside the range of what is expected and unlike the other data. These are called **outliers** and often machine learning modeling and model performance in general can be improved by understanding and even removing these outlier values.\n",
    "\n",
    "<a id=\"what\"></a>\n",
    "<h3 style=\"text-decoration:underline\">What are Outliers?</h3>\n",
    "\n",
    "An outlier is an observation that is unlike the other observations. They are rare, distinct, or do not fit in some way. Outliers can have many causes, such as:\n",
    "- Measurement or input error.\n",
    "- Data corruption.\n",
    "- True outlier observation.\n",
    "\n",
    "There is no precise way to define and identify outliers in general because of the specifics of each dataset. Instead, a domain expert should interpret the raw observations and decide whether a value is an outlier or not. Nevertheless, we can use statistical methods to identify observations that appear to be rare or unlikely given the available data. This does not mean that the values identified are outliers and should be removed, but rather shed light on rare events that may require a second look.\n",
    ">A good tip is to consider plotting the identified outlier values, perhaps in the context of non-outlier values to see if there are any systematic relationships or pattern to the outliers. If there is, perhaps they are not outliers and can be explained, or perhaps the outliers themselves can be identified more systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399e367",
   "metadata": {},
   "source": [
    "<a id=\"handle\"></a>\n",
    "<h3 style=\"text-decoration:underline\">How To Handle Outliers</h3>\n",
    "\n",
    "There are three common strategies to handle outliers:\n",
    "\n",
    "1. Remove the outlier values from the dataframe.\n",
    "2. Mark the outliers and include it as a feature.\n",
    "3. Transform the feature to dampen the effect of the outlier.\n",
    "\n",
    "Similar to detecting outliers, there is no hard-and-fast rule for handling them. How we handle them should be based on two aspects. First, we should consider what makes them an outlier. If we believe they are errors in the data then we might drop the observation or replace outlier values with NaN since we can’t believe those values. However, if we believe the outliers are genuine extreme values then marking them as outliers or transforming their values is more appropriate.\n",
    "\n",
    "Second, how we handle outliers should be based on the goal for machine learning. For example, if the aim is to predict house prices based on features, we may want to include the outliers. On the other hand, if we are trying to predict home loan applications, we may not want to include extreme values.\n",
    "\n",
    ">**So what should we do if we have outliers?** Think about why they are outliers, have an end goal in mind for the data, and, most importantly, remember that not making a decision to address outliers is itself a decision with implications. \n",
    ">- If outliers are present, standardization might not be appropriate because the mean and variance might be highly influenced by the outliers. In this case, use a rescaling method more robust against outliers like `RobustScaler`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be9f6b6",
   "metadata": {},
   "source": [
    "<a id=\"detect\"></a>\n",
    "<h3 style=\"text-decoration:underline\">Detecting Outliers</h3>\n",
    "\n",
    "<h5 style=\"text-decoration:underline\">Elliptic Envelope Method</h5>\n",
    "\n",
    "Detecting outliers is unfortunately more of an art than a science. However, a common method is to assume the data is normally distributed and based on that assumption “draw” an ellipse around the data, classifying any observation inside the ellipse as an inlier (labeled as 1) and any observation outside the ellipse as an outlier (labeled as -1).\n",
    "```python\n",
    "# Create detector\n",
    "outlier_detector = EllipticEnvelope(contamination=.1)\n",
    "\n",
    "# Fit detector\n",
    "outlier_detector.fit(features)\n",
    "\n",
    "# Predict outliers\n",
    "outlier_detector.predict(features)\n",
    "```\n",
    "A major limitation of this approach is the need to specify a `contamination` parameter, which is the proportion of observations that are outliers — a value that we don’t know. Think of contamination as an estimate of the cleanliness of the data. If we expect the data to have few outliers, we can set contamination to something small. However, if we believe that the data is very likely to have outliers, we can set it to a higher value.\n",
    "\n",
    "Instead of looking at observations as a whole, we can instead look at individual features and identify extreme values in those features using interquartile range (IQR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140e9a8",
   "metadata": {},
   "source": [
    "<a id=\"iqr\"></a>\n",
    "<h5 style=\"text-decoration:underline\">Interquartile Range Method</h5>\n",
    "\n",
    "Not all data is normal or normal enough to treat it as being drawn from a Gaussian distribution. A good statistic for summarizing a non-Gaussian distribution sample of data is the Interquartile Range, or IQR for short. \n",
    "\n",
    "The IQR is calculated as the difference between the 75th and the 25th percentiles of the data and defines the box in a box and whisker plot. Remember that percentiles can be calculated by sorting the observations and selecting values at specific indices. The 50th percentile is the middle value, or the average of the two middle values for an even number of examples. We refer to the percentiles as quartiles (quart meaning 4) because the data is divided into four groups via the 25th, 50th and 75th values. The IQR defines the middle 50 percent of the data, or the body of the data. \n",
    "\n",
    "Think of IQR as the spread of the bulk of the data, with outliers being\n",
    "observations far from the main concentration of data. The IQR can be used to identify outliers by defining limits on the sample values that are a factor k of the IQR below the 25th percentile or above the 75th percentile. The common value for the factor k is the value 1.5. A factor k of 3 or more can be used to identify values that are extreme outliers or far outs when described in the context of box and whisker plots. On a box and whisker plot, these limits are drawn as fences on the whiskers (or the lines) that are drawn from the box. Values that fall outside of these values are drawn as dots. We can calculate the percentiles of a dataset using the percentile() NumPy function that takes the dataset and specification of the desired percentile. The IQR can then be calculated as the difference between the 75th and 25th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate interquartile range\n",
    "q25, q75 = np.percentile(data, 25), np.percentile(data, 75)\n",
    "iqr = q75 - q25\n",
    "print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n",
    "\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 1.5\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "\n",
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "\n",
    "# remove outliers\n",
    "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
    "print('Non-outlier observations: %d' % len(outliers_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f5567e",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create a function to return index of outliers\n",
    "def indicies_of_outliers(x):\n",
    "    q1, q3 = np.percentile(x, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (iqr * 1.5)\n",
    "    upper_bound = q3 + (iqr * 1.5)\n",
    "    return np.where((x > upper_bound) | (x < lower_bound))\n",
    "```\n",
    "The approach can be used for multivariate data by calculating the limits on each variable in the dataset in turn, and taking outliers as observations that fall outside of the rectangle or hyper-rectangle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6e71b",
   "metadata": {},
   "source": [
    "<a id=\"std\"></a>\n",
    "<h5 style=\"text-decoration:underline\">Standard Deviation Method</h5>\n",
    "\n",
    "Let’s define a dataset we can use to test the methods. We will generate a population of 10,000 random numbers drawn from a Gaussian distribution with a mean of 50 and a standard deviation of 5. Numbers drawn from a Gaussian distribution will have outliers. That is, by virtue of the distribution itself, there will be a few values that will be a long way from the mean, rare values that we can identify as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e19399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random number generator\n",
    "np.random.seed(1)\n",
    "# generate univariate observations\n",
    "data = 5 * np.random.randn(10000) + 50\n",
    "# summarize\n",
    "print('mean=%.3f stdv=%.3f' % (np.mean(data), np.std(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ffde9",
   "metadata": {},
   "source": [
    "If we know that the distribution of values in the sample is Gaussian or Gaussian-like, we can use the standard deviation of the sample as a cut-off for identifying outliers. The Gaussian distribution has the property that the standard deviation from the mean can be used to reliably summarize the percentage of values in the sample. \n",
    "- 1 Standard Deviation from the Mean: 68 percent.\n",
    "- 2 Standard Deviations from the Mean: 95 percent.\n",
    "- 3 Standard Deviations from the Mean: 99.7 percent.\n",
    "\n",
    "So, if the mean is 50 and the standard deviation is 5, then all data in the sample between 45 and 55 will account for about 68 percent of the data sample. \n",
    "\n",
    "A value that falls outside of 3 standard deviations is part of the distribution, but it is an unlikely or rare event. Three standard deviations from the mean is a common cut-off in practice for identifying outliers in a Gaussian or Gaussian-like distribution. For smaller samples of data, perhaps a value of 2 standard deviations (95 percent) can be used, and for larger samples, perhaps a value of 4 standard deviations (99.9 percent) can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate summary statistics\n",
    "data_mean, data_std = np.mean(data), np.std(data)\n",
    "\n",
    "# define outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "\n",
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "\n",
    "# remove outliers\n",
    "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
    "print('Non-outlier observations: %d' % len(outliers_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dba4b6",
   "metadata": {},
   "source": [
    "You can use the same approach if you have multivariate data, e.g. data with multiple variables, each with a different Gaussian distribution. You can imagine bounds in two dimensions that would define an ellipse if you have two variables. Observations that fall outside of the ellipse would be considered outliers. In three dimensions, this would be an ellipsoid, and so on into higher dimensions. Alternately, if you knew more about the domain, perhaps an outlier may be identified by exceeding the limits on one or a subset of the data dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511b443",
   "metadata": {},
   "source": [
    "<a id=\"auto\"></a>\n",
    "<h5 style=\"text-decoration:underline\">Automatic Outlier Detection</h5>\n",
    "\n",
    "In machine learning, an approach to tackling the problem of outlier detection is one-class classification. A one-class classifier aims at capturing characteristics of training instances, in order to be able to distinguish between them and potential outliers to appear.\n",
    "\n",
    "A simple approach to identifying outliers is to locate those examples that are far from the other examples in the multi-dimensional feature space. This can work well for feature spaces with low dimensionality (few features), although it can become less reliable as the number of features is increased, referred to as the curse of dimensionality. The local outlier factor, or LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers. The scikit-learn library provides an implementation of this approach in the `LocalOutlierFactor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1866cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/Data Cleaning/boston_housing.csv',\n",
    "                 names=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "                        'DIS','RAD','TAX', 'PTRATIO', 'B', 'LSTAT','MEDV'])\n",
    "\n",
    "X = df.drop(columns=['MEDV'])\n",
    "y = df['MEDV']\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=1)\n",
    "\n",
    "# summarize the shape of the train and test sets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89306697",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8405989d",
   "metadata": {},
   "source": [
    "It is a regression predictive modeling problem, meaning that we will be predicting a numeric value. All input variables are also numeric. In this case, we will fit a linear regression algorithm and evaluate model performance by training the model on the test dataset and making a prediction on the test data and evaluate the predictions using the mean absolute error (MAE). \n",
    "\n",
    "We can also try removing outliers from the training dataset. The expectation is that the outliers are causing the linear regression model to learn a bias or skewed understanding of the problem, and that removing these outliers from the training set will allow a more effective model to be learned. We can achieve this by defining the `LocalOutlierFactor` model and using it to make a prediction on the training dataset, marking each row in the training dataset as normal (1) or an outlier (-1). We will use the default hyperparameters for the outlier detection model, although it is a good idea to tune the configuration to the specifics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd8a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "full_model = LinearRegression()\n",
    "full_model.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = full_model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('The MAE with the full dataset: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985a763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat_new = lof.fit_predict(X_train)\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = (yhat_new != -1)\n",
    "X_train_out, y_train_out = X_train[mask], y_train[mask]\n",
    "print(X_train_out.shape, y_train_out.shape)\n",
    "\n",
    "# fit the model\n",
    "model_out = LinearRegression()\n",
    "model_out.fit(X_train_out, y_train_out)\n",
    "\n",
    "# evaluate the model\n",
    "yhat_out = model_out.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat_out)\n",
    "print('The MAE after removing the outliers: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05365644",
   "metadata": {},
   "source": [
    "Firstly, we can see that the number of examples in the training dataset has been reduced from 339 to 305, meaning 34 rows containing outliers were identified and deleted. We can also see a reduction in MAE from about 3.417 by a model fit on the entire training dataset, to about 3.356 on a model fit on the dataset with outliers removed.\n",
    "\n",
    "The Scikit-Learn library provides other outlier detection algorithms that can be used in the same way such as:\n",
    "- `sklearn.ensemble.IsolationForest`\n",
    "- `sklearn.svm.OneClassSVM` - Unsupervised Outlier Detection. Estimate the support of a high-dimensional distribution. The implementation is based on libsvm.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/outlier_detection.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f16e01",
   "metadata": {},
   "source": [
    "<a id=\"missing\"></a>\n",
    "<h1 align=\"center\">HOW TO HANDLE MISSING DATA</h1>\n",
    "\n",
    "Most machine learning algorithms cannot handle any missing values in the target and feature arrays. For this reason, we cannot ignore missing values in our data and must address the issue during preprocessing. Most Real-world data has missing values, with the likelihood increasing with the size of the dataset. As such, it is good practice to identify and replace missing values for each column in the input data prior to modeling. This is called ***missing data imputation*** (or ***imputing***). \n",
    "\n",
    "The simplest solution is to delete every observation that contains one or more missing values, a task quickly and easily accomplished using NumPy or pandas. That said, we should be very reluctant to delete observations with missing values. Deleting them is the nuclear option, since the algorithm loses access to the information contained in the observation’s non-missing values. Depending on the cause of the missing values, deleting observations can introduce bias into the data. \n",
    "\n",
    "There are three types of missing data:\n",
    "1. **Missing Completely At Random (MCAR)** - The probability that a value is missing is independent of everything. \n",
    "2. **Missing At Random (MAR)** - The probability that a value is missing is not completely random, but depends on the information captured in other features. \n",
    "3. **Missing Not At Random (MNAR)** - The probability that a value is missing is not random and depends on information not captured in our features.\n",
    "\n",
    "It is sometimes acceptable to delete observations if they are MCAR or MAR. However, if the value is MNAR, the fact that a value is missing is itself information. Deleting MNAR observations can inject bias into our data because we are removing observations produced by some unobserved systematic effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a8b7c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose_conc</th>\n",
       "      <th>blood_press</th>\n",
       "      <th>tricep_thick</th>\n",
       "      <th>serum_ins</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>768.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.85</td>\n",
       "      <td>120.89</td>\n",
       "      <td>69.11</td>\n",
       "      <td>20.54</td>\n",
       "      <td>79.80</td>\n",
       "      <td>31.99</td>\n",
       "      <td>0.47</td>\n",
       "      <td>33.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.37</td>\n",
       "      <td>31.97</td>\n",
       "      <td>19.36</td>\n",
       "      <td>15.95</td>\n",
       "      <td>115.24</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0.33</td>\n",
       "      <td>11.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.30</td>\n",
       "      <td>0.24</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.00</td>\n",
       "      <td>117.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>30.50</td>\n",
       "      <td>32.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.00</td>\n",
       "      <td>140.25</td>\n",
       "      <td>80.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>127.25</td>\n",
       "      <td>36.60</td>\n",
       "      <td>0.63</td>\n",
       "      <td>41.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.00</td>\n",
       "      <td>199.00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>846.00</td>\n",
       "      <td>67.10</td>\n",
       "      <td>2.42</td>\n",
       "      <td>81.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pregnant  glucose_conc  blood_press  tricep_thick  serum_ins     bmi  \\\n",
       "count    768.00        768.00       768.00        768.00     768.00  768.00   \n",
       "mean       3.85        120.89        69.11         20.54      79.80   31.99   \n",
       "std        3.37         31.97        19.36         15.95     115.24    7.88   \n",
       "min        0.00          0.00         0.00          0.00       0.00    0.00   \n",
       "25%        1.00         99.00        62.00          0.00       0.00   27.30   \n",
       "50%        3.00        117.00        72.00         23.00      30.50   32.00   \n",
       "75%        6.00        140.25        80.00         32.00     127.25   36.60   \n",
       "max       17.00        199.00       122.00         99.00     846.00   67.10   \n",
       "\n",
       "       pedigree     age  \n",
       "count    768.00  768.00  \n",
       "mean       0.47   33.24  \n",
       "std        0.33   11.76  \n",
       "min        0.08   21.00  \n",
       "25%        0.24   24.00  \n",
       "50%        0.37   29.00  \n",
       "75%        0.63   41.00  \n",
       "max        2.42   81.00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diab = pd.read_csv('./datasets/Data Cleaning/Diabetes.csv',header=None)\n",
    "df_diab.columns = ['pregnant','glucose_conc','blood_press','tricep_thick',\n",
    "                  'serum_ins','bmi','pedigree','age','target']\n",
    "\n",
    "df_diab.drop('target',axis=1).describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aeab91",
   "metadata": {},
   "source": [
    "<a id=\"identify\"></a>\n",
    "<h3 style=\"text-decoration:underline\">Identify & Mark Missing Values</h3>\n",
    "\n",
    "Missing values are identified within rows of data where one or more values or columns in that row are not present. The values may be missing completely or they may be marked with a special character or value. No matter how they appear, knowing what to expect and checking to make sure the data matches that expectation will reduce future problems.<br>\n",
    ">**Missing values are frequently indicated by out-of-range entries; perhaps a negative number (e.g., -1) in a numeric field that is normally only positive, or a 0 in a numeric field that can never normally be 0.**\n",
    ">>**Values could be missing for many reasons, often specific to the problem domain, and might include reasons such as corrupt measurements or data unavailability.**\n",
    "\n",
    "From the descriptive statistics above, We can see that there are columns that have a minimum value of zero (0). On most of these columns, a value of zero does not make sense and indicates an invalid or missing value. Specifically, the following columns have an invalid zero minimum value: glucose_conc, blood_press, tricep_thick, serum_ins and bmi. We can corroborate this by the definition of those columns and the domain knowledge that a zero value is invalid for those measures, e.g. a zero for body mass index or blood pressure is invalid.\n",
    ">**In Python, specifically Pandas, NumPy and Scikit-Learn, we mark missing values as NaN.** Values with a NaN value are ignored from operations like sum, count, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_zeros = ['glucose_conc', 'blood_press', 'tricep_thick',\n",
    "                   'serum_ins', 'bmi']\n",
    "\n",
    "# Using pandas .where\n",
    "# df_diab[cols_with_zeros].where(df_diab[cols_with_zeros]!=0,np.nan)\n",
    "# df_diab[cols_with_zeros].where(df_diab[cols_with_zeros]!=0)\n",
    "\n",
    "\n",
    "# Using pandas .mask\n",
    "# df_diab[cols_with_zeros].mask(df_diab[cols_with_zeros]==0,np.nan)\n",
    "# df_diab[cols_with_zeros].mask(df_diab[cols_with_zeros]==0)\n",
    "\n",
    "# Using pandas .replace\n",
    "# df_diab[cols_with_zeros] = df_diab[cols_with_zeros].replace(0,np.nan)\n",
    "df_diab[cols_with_zeros] = df_diab[cols_with_zeros].replace({0:np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4041a4",
   "metadata": {},
   "source": [
    "Now that the missing values have been replaced by NaN, detecting missing values is easier via Pandas. \n",
    "- `isnull()`|`isna()` - detects missing values\n",
    "- `notnull()`|`notna()` - detects non-missing values\n",
    "- `count()` - counts the non-NA cells for each column or row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a series\n",
    "null_count = df_diab.isnull().sum()\n",
    "\n",
    "null_count[null_count>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a dataframe\n",
    "\n",
    "null_df = pd.DataFrame(df_diab.isna().sum(), columns=['Null_Count'])\n",
    "null_df.index.name = 'Column'\n",
    "# null_df.sort_values(['Null_Count'], ascending=False)\n",
    "null_df[null_df['Null_Count']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff164483",
   "metadata": {},
   "source": [
    "We can see that columns at index 1, 2 and 5 have just a few zero values, whereas columns 3 and 4 show a lot more, nearly half of the rows. This highlights that different missing value strategies may be needed for different columns, e.g. to ensure that there are still a sufficient number of records left to train a predictive model.\n",
    "\n",
    "There are also many different visualizations for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa6681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(df_diab.isnull(),cbar=False,cmap='viridis')\n",
    "\n",
    "# msno.bar(df_diab)\n",
    "# msno.matrix(df_diab)\n",
    "msno.heatmap(df_diab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dcb82d",
   "metadata": {},
   "source": [
    "The heatmap is used to identify correlations of the nullity between each of the different columns. In other words, it can be used to identify if there is a relationship in the presence of null values between each of the columns.\n",
    "- Values close to positive 1 indicate that the presence of null values in one column is correlated with the presence of null values in another column.\n",
    "- Values close to negative 1 indicate that the presence of null values in one column is anti-correlated with the presence of null values in another column. In other words, when null values are present in one column, there are data values present in the other column, and vice versa.\n",
    "- Values close to 0, indicate there is little to no relationship between the presence of null values in one column compared to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5084768",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.dendrogram(df_diab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0eaf2",
   "metadata": {},
   "source": [
    "The dendrogram plot provides a tree-like graph generated through hierarchical clustering and groups together columns that have strong correlations in nullity. If a number of columns are grouped together at level zero, then the presence of nulls in one of those columns is directly related to the presence or absence of nulls in the others columns. The more separated the columns in the tree, the less likely the null values can be correlated between the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf692d3",
   "metadata": {},
   "source": [
    "Finding missing data is the easy part! Determining what to do next is more complicated. Typically, we are most interested in knowing why we are missing data. Once we know what 'type of missingness' we have (the source of missing data), we can proceed effectively. How we fill in data depends largely on why it is missing (types of missingness) and what sampling we have available to us. The most common options are:\n",
    "- **Delete missing data altogether**\n",
    "- **Fill in missing data**\n",
    "- **Collect more data**\n",
    "\n",
    ">**When using imputation, it is a good idea to create a binary feature indicating whether or not the observation contains an imputed value.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba23d43",
   "metadata": {},
   "source": [
    "<a id=\"remove\"></a>\n",
    "<h3><ins>Remove Rows With Missing Values</ins></h3>\n",
    "\n",
    "The simplest strategy for handling missing data is to remove records that contain a missing value. This is as simple as using Pandas `dropna` function along with the axis argument. By default, axis=0, i.e., along row, which means that if any value within a row is NA then the whole row is excluded.\n",
    ">**Though not always recommended, it is okay to drop missing rows if less than 5%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19447124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original shape including missing values\n",
    "\n",
    "df_diab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new shape after removing all rows with missing values in them\n",
    "\n",
    "df_diab.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8640dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diab.dropna(inplace=True)\n",
    "X = df_diab.drop(columns=['target'])\n",
    "y = df_diab['target']\n",
    "\n",
    "# define the model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# define the model evaluation procedure\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "result = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "# report the mean performance\n",
    "print('Accuracy: %.3f' % np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01820385",
   "metadata": {},
   "source": [
    "Removing rows with missing values can be too limiting on some predictive modeling problems. An alternative is to **impute missing values**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d482f2d",
   "metadata": {},
   "source": [
    "<a id=\"impute\"></a>\n",
    "<h3><ins>How to Use Statistical Imputation</ins></h3>\n",
    "\n",
    "A simple and popular approach to data imputation involves using statistical methods to estimate a value for a column from those values that are present, then replace all missing values in the column with the calculated statistic. It is simple because statistics are fast to calculate and it is popular because it often proves very effective. Some common statistics are:\n",
    "- mean\n",
    "- median\n",
    "- mode\n",
    "- constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42423fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset - missing values are maarked with a ?\n",
    "df_horse = pd.read_csv('./datasets/Data Cleaning/horse_colic.csv',\n",
    "                       header=None, na_values='?')\n",
    "\n",
    "df_horse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d545d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize the number of rows with missing values for each column\n",
    "for i in range(df_horse.shape[1]):\n",
    "    # count number of rows with missing values\n",
    "    n_miss = df_horse[[i]].isnull().sum()\n",
    "    perc = n_miss / df_horse.shape[0] * 100\n",
    "    print('> Column %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281d5f8",
   "metadata": {},
   "source": [
    "<a id=\"fill\"></a>\n",
    "<h5 style=\"text-decoration:underline\">Using Pandas to Fill Missing Values</h5>\n",
    "\n",
    "Pandas provides various methods for cleaning the missing values. In order to fill null values in a dataset, we use `fillna()`, `replace()` and `interpolate()` to replace NaN values with some value of their own. All these functions help in filling a null value in a dataset of a DataFrame. `Interpolate()` function is basically used to fill NA values in the dataframe but it uses various interpolation techniques to fill the missing values rather than hard-coding the value.\n",
    "\n",
    "```python\n",
    "# reference a single column\n",
    "df_horse['column_name'].fillna(value=   )\n",
    "\n",
    "# A better way is:\n",
    "df_horse.fillna(value={'column_name': value_to_replace})\n",
    "\n",
    "# not good for changing multiple columns at a time:\n",
    "df_horse[['column_name1', 'column_name2']].fillna(value=   )\n",
    "\n",
    "# better to do this for multiple columns:\n",
    "df_horse.fillna(value={'column_name1': value_to_replace, \n",
    "                       'column_name2': df_horse['column_name3'].mean()\n",
    "                      })\n",
    "```\n",
    "\n",
    "Many times, we have to replace a generic value with some specific value. We can achieve this by applying the replace method. Replacing NA with a scalar value is equivalent behavior of the fillna() function.\n",
    "```python\n",
    "df_horse.replace(to_replace=None, value=None, inplace=False, limit=None,\n",
    "                 regex=False, method='pad')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac75be9",
   "metadata": {},
   "source": [
    "<a id=\"transform\"></a>\n",
    "<h5 style=\"text-decoration:underline\">SimpleImputer Data Transform</h5>\n",
    "\n",
    "The `SimpleImputer` in scikit-learn's machine learning library is a data transform that is first configured based on the type of statistic to calculate for each column, e.g. mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e82968",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_horse.drop(23,axis=1)\n",
    "y = df_horse[23]\n",
    "\n",
    "# summarize total missing\n",
    "print('Original - Missing: %d' % sum(np.isnan(X.values).flatten()))\n",
    "\n",
    "# define imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset \n",
    "Xtrans = imputer.transform(X)\n",
    "\n",
    "# summarize total missing -> Xtrans are values only so no need for .values\n",
    "print('After imputing - Missing: %d' % sum(np.isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836499ec",
   "metadata": {},
   "source": [
    "<a id=\"eval\"></a>\n",
    "<h5 style=\"text-decoration:underline\">SimpleImputer and Model Evaluation</h5>\n",
    "\n",
    "It is a good practice to evaluate machine learning models on a dataset using k-fold cross validation. To correctly apply statistical missing data imputation and avoid data leakage, it is required that the statistics calculated for each column are calculated on the training dataset only, then applied to the train and test sets for each fold in the dataset. This can be achieved by creating a modeling pipeline where the first step is the statistical imputation, then the second step is the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cfcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6c8d1",
   "metadata": {},
   "source": [
    "The pipeline is evaluated using three repeats of 10-fold cross-validation and reports the mean classification accuracy on the dataset as about 86%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49abd0",
   "metadata": {},
   "source": [
    "<a id=\"compare\"></a>\n",
    "<h5 style=\"text-decoration:underline\">Comparing Different Imputed Statistics</h5>\n",
    "\n",
    "How do we know that using a ‘mean’ statistical strategy is good or best for this dataset? The answer is that we don’t and that it was chosen arbitrarily. We can design an experiment to test each statistical strategy and discover what works best for this dataset, comparing the mean, median, mode (most frequent), and constant strategies. The mean accuracy of each approach can then be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0034f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare statistical imputation strategies for the horse colic dataset\n",
    "\n",
    "# evaluate each strategy on the dataset\n",
    "results = list()\n",
    "strategies = ['mean', 'median', 'most_frequent', 'constant']\n",
    "\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)), \n",
    "                               ('m', RandomForestClassifier())])\n",
    "\n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='accuracy',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "\n",
    "    # store results\n",
    "    results.append(scores)\n",
    "\n",
    "    print('>%s %.3f (%.3f)' % (s, np.mean(scores), np.std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=strategies, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80bdf05",
   "metadata": {},
   "source": [
    "In this case, the results suggest that using a constant value, e.g. 0, results in the best performance of about 87.7 percent. We can see that the distribution of accuracy scores for the constant strategy may be better than the other strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4608e",
   "metadata": {},
   "source": [
    "<a id=\"predict\"></a>\n",
    "<h5 style=\"text-decoration:underline\">SimpleImputer Transform When Making a Prediction</h5>\n",
    "\n",
    "We may wish to create a final modeling pipeline with the constant imputation strategy and random forest algorithm, then make a prediction for new data. This can be achieved by defining the pipeline and fitting it on all available data, then calling the `predict()` function and passing new data in as an argument. ***Importantly, the row of new data must mark any missing values using the NaN value.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('i', SimpleImputer(strategy='constant')), \n",
    "                           ('m', RandomForestClassifier())])\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# define new (made-up in this case) data\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, np.nan, 2, 5, 4, 4, np.nan, \n",
    "       np.nan, np.nan, 3, 5, 45.00, 8.40, np.nan, np.nan, 2, 11300,\n",
    "       0, 0, 2]\n",
    "\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row])\n",
    "\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c57381",
   "metadata": {},
   "source": [
    "<a id=\"knn\"></a>\n",
    "<h3><ins>How to Use KNN Imputation</ins></h3>\n",
    "\n",
    "Another popular approach to missing data imputation is to use a model to predict the missing values. This requires a model to be created for each input variable that has missing values. If input variables are numeric, then regression models can be used for prediction, and this case is quite common. Although any one among a range of different models can be used to predict the missing values, the **k-nearest neighbor (KNN)** algorithm has proven to be generally effective, often referred to as nearest neighbor imputation.  Configuration of KNN imputation often involves selecting the distance measure (e.g. Euclidean) and the number of contributing neighbors for each prediction, the k hyperparameter of the KNN algorithm.\n",
    "\n",
    "The scikit-learn machine learning library provides the **`KNNImputer`** class that supports nearest neighbor imputation. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors. Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed. When the number of available neighbors is less than n_neighbors and there are no defined distances to the training set, the training set average for that feature is used during imputation. If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during transform.\n",
    "\n",
    ">**It is often the case that KNN imputing will produce better results than the `SimpleImputer` module which fills in missing values with the feature’s mean, median, or most frequent value.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87397f50",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "<h5 style=\"text-decoration:underline\">KNNImputer Data Transform</h5>\n",
    "\n",
    "The `KNNImputer` is a data transform that is first configured based on the method used to estimate the missing values. The default distance measure is a *Euclidean* distance measure that is NaN aware, e.g. will not include NaN values when calculating the distance between members of the training dataset. This is set via the `metric` argument. The number of neighbors is set to five by default and can be configured by the `n neighbors` argument. Finally, the distance measure can be weighed proportional to the distance between instances (rows), although this is set to a uniform weighting by default, controlled via the `weights` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39511e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize total missing\n",
    "print('Original - Missing: %d' % sum(np.isnan(X.values).flatten()))\n",
    "\n",
    "# define imputer\n",
    "imputer = KNNImputer()\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "\n",
    "# summarize total missing -> Xtrans are values only so no need for .values\n",
    "print('After KNN imputing - Missing: %d' % sum(np.isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab6638",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "<h5 style=\"text-decoration:underline\">KNNImputer and Model Evaluation</h5>\n",
    "\n",
    "It is a good practice to evaluate machine learning models on a dataset using k fold crossvalidation. To correctly apply nearest neighbor missing data imputation and avoid data leakage, it is required that the models calculated for each column are calculated on the training dataset only, then applied to the train and test sets for each fold in the dataset. This can be achieved by creating a modeling pipeline where the first step is the nearest neighbor imputation, then the second step is the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define modeling pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = KNNImputer(missing_values=np.nan, n_neighbors=5,\n",
    "                     weights='uniform', metric='nan_euclidean')\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520df25",
   "metadata": {},
   "source": [
    "How do we know that using a default number of neighbors of five is good or best for this dataset? The answer is that we don’t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44b818",
   "metadata": {},
   "source": [
    "<a id=\"range\"></a>\n",
    "<h5 style=\"text-decoration:underline\">KNNImputer and Different Number of Neighbors</h5>\n",
    "\n",
    "The key hyperparameter for the KNN algorithm is k; that controls the number of nearest neighbors that are used to contribute to a prediction. It is good practice to test a suite of different values for k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e75cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each strategy on the dataset\n",
    "results = list()\n",
    "strategies = [str(i) for i in [1,3,5,7,9,15,18,21]]\n",
    "\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=int(s))),\n",
    "                               ('m', RandomForestClassifier())])\n",
    "    \n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='accuracy', \n",
    "                             cv=cv, n_jobs=-1)\n",
    "\n",
    "    # store results\n",
    "    results.append(scores)\n",
    "    print('> k=%s %.3f (%.3f)' % (s, np.mean(scores), np.std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=strategies, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0f397",
   "metadata": {},
   "source": [
    "The plot suggest that there is not much difference in the k value when imputing the missing values, with minor fluctuations around the mean performance (green triangle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41609c83",
   "metadata": {},
   "source": [
    "<a id=\"make\"></a>\n",
    "<h5 style=\"text-decoration:underline\">KNNImputer Transform When Making a Prediction</h5>\n",
    "\n",
    "We may wish to create a final modeling pipeline with the nearest neighbor imputation strategy and random forest algorithm, then make a prediction for new data. This can be achieved by defining the pipeline and fitting it on all available data, then calling the `predict()` function and passing new data in as an argument. ***Importantly, the row of new data must mark any missing values using the NaN value.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6006e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the modeling pipeline\n",
    "pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=21)), \n",
    "                           ('m', RandomForestClassifier())])\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# define new data\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, np.nan, 2, 5, 4, 4, np.nan, \n",
    "       np.nan, np.nan, 3, 5, 45.00, 8.40, np.nan, np.nan, 2, 11300, 0, 0, 2]\n",
    "\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row])\n",
    "\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d2d11",
   "metadata": {},
   "source": [
    "<a id=\"iter\"></a>\n",
    "<h3><ins>How to Use Iterative Imputation</ins></h3>\n",
    "\n",
    "A sophisticated approach to imputing missing values is to use an **iterative imputation** model. Iterative imputation refers to a process where each feature is modeled as a function of the other features, e.g. a regression problem where missing values are predicted. Each feature is imputed sequentially, one after the other, allowing prior imputed values to be used as part of a model in predicting subsequent features.\n",
    "\n",
    "It is iterative because this process is repeated multiple times, allowing ever improved estimates of missing values to be calculated as missing values across all features are estimated. This approach may be generally referred to as fully conditional specification (FCS) or multivariate imputation by chained equations (MICE).\n",
    "\n",
    "Different regression algorithms can be used to estimate the missing values for each feature, although linear methods are often used for simplicity. The number of iterations of the procedure is often kept small, such as 10. Finally, the order that features are processed sequentially can be considered, such as from the feature with the least missing values to the feature with the most missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48758428",
   "metadata": {},
   "source": [
    "<a id=\"iidt\"></a>\n",
    "<h5 style=\"text-decoration:underline\">IterativeImputer Data Transform</h5>\n",
    "\n",
    "`IterativeImputer` is a data transform that is first configured based on the method used to estimate the missing values. By default, a BayesianRidge model is employed that uses a function of all other input features. Features are filled in ascending order, from those with the fewest missing values to those with the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba8b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize total missing\n",
    "print('Original - Missing: %d' % sum(np.isnan(X.values).flatten()))\n",
    "\n",
    "# define imputer\n",
    "imputer = IterativeImputer()\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "\n",
    "# summarize total missing -> Xtrans are values only so no need for .values\n",
    "print('After iterative imputing - Missing: %d' % sum(np.isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7adb91e",
   "metadata": {},
   "source": [
    "<a id=\"ii_model\"></a>\n",
    "<h5 style=\"text-decoration:underline\">IterativeImputer and Model Evaluation</h5>\n",
    "\n",
    "It is a good practice to evaluate machine learning models on a dataset using k fold crossvalidation. To correctly apply iterative missing data imputation and avoid data leakage, it is required that the models for each column are calculated on the training dataset only, then applied to the train and test sets for each fold in the dataset. This can be achieved by creating a modeling pipeline where the first step is the iterative imputation, then the second step is the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d779a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define modeling pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = IterativeImputer()\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9acfadb",
   "metadata": {},
   "source": [
    "The pipeline is evaluated using three repeats of 10-fold cross-validation and reports the mean classification accuracy on the dataset as about 87%.\n",
    "\n",
    "How do we know that using a default iterative strategy is good or best for this dataset? The answer is that we don’t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6608fcf7",
   "metadata": {},
   "source": [
    "<a id=\"order\"></a>\n",
    "<h5 style=\"text-decoration:underline\">IterativeImputer and Different Imputation Order</h5>\n",
    "\n",
    "By default, imputation is performed in ascending order from the feature with the least missing values to the feature with the most. This makes sense as we want to have more complete data when it comes time to estimating missing values for columns where the majority of values are missing. Nevertheless, we can experiment with different imputation order strategies, such as descending, right-to-left (Arabic), left-to-right (Roman), and random. The example below evaluates and compares each available imputation order configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each strategy on the dataset\n",
    "results = list()\n",
    "strategies = ['ascending', 'descending', 'roman', 'arabic', 'random']\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(steps=[('i', IterativeImputer(imputation_order=s)),\n",
    "                               ('m', RandomForestClassifier())])\n",
    "\n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='accuracy',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "\n",
    "    # store results\n",
    "    results.append(scores)\n",
    "\n",
    "    print('>Strategy: %s %.3f (%.3f)' % (s, np.mean(scores), np.std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=strategies, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978224d4",
   "metadata": {},
   "source": [
    "<a id=\"number\"></a>\n",
    "<h5 style=\"text-decoration:underline\">IterativeImputer and Different Number of Iterations</h5>\n",
    "\n",
    "By default, the `IterativeImputer` will repeat the number of iterations 10 times. It is possible that a large number of iterations may begin to bias or skew the estimate and that few iterations may be preferred. The number of iterations of the procedure can be specified via the **`max_iter`** argument. It may be interesting to evaluate different numbers of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "strategies = [i for i in range(1, 21)]\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(steps=[('i', IterativeImputer(max_iter=s)),\n",
    "                               ('m', RandomForestClassifier())])\n",
    "\n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='accuracy',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "\n",
    "    # store results\n",
    "    results.append(scores)\n",
    "    print('>max_iter=%s %.3f (%.3f)' % (s, np.mean(scores), np.std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=strategies, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e81b3d",
   "metadata": {},
   "source": [
    "<a id=\"ii_pred\"></a>\n",
    "<h5 style=\"text-decoration:underline\">IterativeImputer Transform When Making a Prediction</h5>\n",
    "\n",
    "We may wish to create a final modeling pipeline with the iterative imputation and random forest algorithm, then make a prediction for new data. This can be achieved by defining the pipeline and fitting it on all available data, then calling the `predict()` function, passing new data in as an argument. Importantly, the row of new data must mark any missing values using the NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33352d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('i', IterativeImputer()), \n",
    "                           ('m', RandomForestClassifier())])\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# define new data\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, np.nan, 2, 5, 4, 4, np.nan, \n",
    "       np.nan, np.nan, 3, 5, 45.00, 8.40, np.nan, np.nan, 2, 11300,\n",
    "       0, 0, 2]\n",
    "\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row])\n",
    "\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299fbb9",
   "metadata": {},
   "source": [
    "<a id=\"ref\"></a>\n",
    "<h3><ins>REFERENCES</ins></h3>\n",
    "\n",
    "<ins>BOOKS</ins>\n",
    "- [Data Preparation for Machine Learning by Jason Brownlee](https://machinelearningmastery.com/data-preparation-for-machine-learning/)\n",
    "- [Machine Learning with Python Cookbook: Practical Solutions from Preprocessing to Deep Learning](https://gaurav320.github.io/vpspu.github.io/eb/pdf/ML.pdf)\n",
    "\n",
    "<ins>ARTICLES / WEBSITES</ins>\n",
    "- [Towards Data Science - Using the missingno Python library to Identify and Visualise Missing Data Prior to Machine Learning](https://towardsdatascience.com/using-the-missingno-python-library-to-identify-and-visualise-missing-data-prior-to-machine-learning-34c8c5b5f009)\n",
    "\n",
    "\n",
    "<ins>PYTHON PACKAGES</ins>\n",
    "- [missingno](https://github.com/ResidentMario/missingno)\n",
    "- [sklearn - Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html#)\n",
    "- [sklearn - Imputation of missing values](https://scikit-learn.org/stable/modules/impute.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b496c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
